hko7: &hko7
  type: hko7
  input_length: 10
  pred_length: 10
  base_freq: 6min

dataset:
  train:
    <<: *hko7

  valid:
    <<: *hko7

sampler:
  type: TrainingSampler

dataloader:
  num_workers: 4
  pin_memory: False
  prefetch_factor: 2
  persistent_workers: True

trainer:
  batch_size: 8
  test_batch_size: 8
  max_epoch: &max_epoch 1
  max_step: 30000

model:
  type: Iter_model
  params:
    sub_model:
      ConvGRU:
        stack_num: 3
        hidden_dims: [32, 64, 64]
        first_conv: [8, 7, 5, 1]
        last_deconv: [1, 7, 5, 1]
        h2h_kernel_size: [[5, 5], [5, 5], [3, 3]]
        h2h_pad: [[2, 2], [2, 2], [1, 1]]
        i2h_kernel_size: [[3, 3], [3, 3], [3, 3]]
        i2h_pad: [[1, 1], [1, 1], [1, 1]]
        downsample: [[5, 3, 1], [3, 2, 1]]
        upsample: [[5, 3, 1], [4, 2, 1]]
        featmap_size: [96, 32, 16]
        residual_connection: True

    save_best: &loss_type MSE
    use_ceph: True
    metrics_list: ["HSS","CSI", "MSE", "MAE"]
    ceph_checkpoint_path: "mpas:s3://mpas/checkpoint"
    data_type: fp32

    optimizer:
      ConvGRU:
        type: AdamW
        params:
          lr: 0.001
          betas: [0.9, 0.95]
          # eps: 0.000001

    lr_scheduler:
      ConvGRU:
        by_step: True
        sched: cosine
        epochs: *max_epoch
        min_lr: 0.
        warmup_lr: 0.
        warmup_epochs: 0.0033
        lr_noise: 
        cooldown_epochs: 0

    extra_params:
      loss_type: MSELoss
      enabled_amp: False
      log_step: 20
      z_score_delta: False


