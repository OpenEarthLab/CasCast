hko7: &hko7
  type: hko7_preprocess
  input_length: 1
  pred_length: 0
  base_freq: 6min

dataset:
  train:
    <<: *hko7

  valid:
    <<: *hko7
  
  test:
    <<: *hko7

sampler:
  type: DistributedSampler

dataloader:
  num_workers: 8 
  pin_memory: False
  prefetch_factor: 2
  persistent_workers: True
  drop_last: False

trainer:
  batch_size: 16 # to check
  valid_batch_size: 16
  test_batch_size: 16
  max_epoch: &max_epoch 1
  max_step: 100000

model:
  type: latent_compress_model
  params:
    latent_size: 60x60x4

    sub_model:
      autoencoder_kl:
        in_channels: 1
        out_channels: 1
        down_block_types: ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D']
        up_block_types: ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D']
        block_out_channels: [128, 256, 512, 512]
        layers_per_block: 2
        latent_channels: 4
        norm_num_groups: 32


    save_best: &loss_type MSE
    use_ceph: True
    ceph_checkpoint_path: "mpas:s3://sevir/checkpoint"
    metrics_type: hko7_official
    hko7_seq_len: 1 ## TODO: check
    data_type: fp32

    visualizer:
      visualizer_type: hko7_visualizer
      visualizer_step: 1000

    optimizer:
      autoencoder_kl:
        type: AdamW
        params:
          lr: 0.001
          betas: [0.9, 0.95]
          # eps: 0.000001
      
    lr_scheduler:
      autoencoder_kl:
        by_step: False
        sched: cosine
        epochs: *max_epoch
        min_lr: 0.00001
        warmup_lr: 0.00001
        warmup_epochs: 0.1
        lr_noise: 
        cooldown_epochs: 0

    extra_params:
      loss_type: MSELoss
      enabled_amp: False
      log_step: 20
      predictor_checkpoint_path: None #EarthFormer_xy/world_size1-xytest/checkpoint_latest.pth ## for pretrained advective predictor
      autoencoder_checkpoint_path: autoencoder_kl_gan/world_size4-hko7_50x50x4_200k/checkpoint_latest.pth ## for pretrained autoencoder