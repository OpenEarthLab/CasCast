sevir: &sevir
  type: sevir_latent25
  input_length: &input_length 13 
  pred_length: &pred_length 12
  total_length: &total_length 25
  base_freq: 5min
  data_dir: radar:s3://sevir_latent
  latent_size: 48x48x4

dataset:
  train:
    <<: *sevir

  valid:
    <<: *sevir

sampler:
  type: TrainingSampler

dataloader:
  num_workers: 8
  pin_memory: False
  prefetch_factor: 2
  persistent_workers: True

trainer:
  batch_size: 8 # to check
  valid_batch_size: 16
  max_epoch: &max_epoch 1
  max_step: 100000


model:
  type: latent_diffusion_joint_model
  params:
    diffusion_kwargs:
        noise_scheduler:
          # DPMSolverMultistepScheduler:
          #   num_train_timesteps: &num_classes 20
          #   beta_start: &sigma_start 0.0001
          #   beta_end: &sigma_end 0.02
          #   beta_schedule: &sigma_dist linear
          DDPMScheduler:
            num_train_timesteps: &num_classes 1000
            beta_start: &sigma_start 0.0001
            beta_end: &sigma_end 0.02
            beta_schedule: &sigma_dist linear
            clip_sample_range: 13
            prediction_type: epsilon
        classifier_free_guidance:
          p_uncond: 0.1
          guidance_weight: 1 ## TODO

    sub_model:
      latentCast_diff_test1:
        arch: DiT-custom
        config:
          input_size: 48
          in_channels: 8
          mlp_ratio: 4.0
          learn_sigma: false
          out_channels: 48
          split_num: 12
          num_heads: 16
          single_heads_num: 4
          hidden_size: 1152
          enc_hidden_size: 256
          patch_size: 2
          enc_depth: 12
          latent_depth: 12
      
      autoencoder_kl:
        in_channels: 1
        out_channels: 1
        down_block_types: ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D']
        up_block_types: ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D']
        block_out_channels: [128, 256, 512, 512]
        layers_per_block: 2
        latent_channels: 4
        norm_num_groups: 32

      SimVP:
        in_shape: [13, 4, 48, 48]
        hid_S: 32
        hid_T: 256
        N_S: 2
        N_T: 6

        model_type: 'gSTA'
        mlp_ratio: 8
        drop: 0.0
        drop_path: 0.0
        spatio_kernel_enc: 3
        spatio_kernel_dec: 3
        act_inplace: True

        out_frames: 12

    save_best: &loss_type MSE
    use_ceph: True
    ceph_checkpoint_path: "mpas:s3://sevir/checkpoint"
    metrics_type: None
    data_type: fp32

    visualizer:
      visualizer_type: sevir_visualizer
      visualizer_step: 2000

    optimizer:
      latentCast_diff_test1:
        type: AdamW
        params:
          lr: 0.0005
          betas: [0.9, 0.95]
          # eps: 0.000001
      
      SimVP:
        type: AdamW
        params:
          lr: 0.001
          betas: [0.9, 0.999]
          weight_decay: 0.00001
          # eps: 0.000001
      
    lr_scheduler:
      latentCast_diff_test1:
        by_step: True
        sched: cosine
        epochs: *max_epoch
        min_lr: 0.00001
        warmup_lr: 0.00001
        warmup_epochs: 0.1
        lr_noise: 
        cooldown_epochs: 0
      
      SimVP:
        by_step: True
        sched: cosine
        epochs: *max_epoch
        min_lr: 0.000001
        warmup_lr: 0.000001
        warmup_epochs: 0.1
        lr_noise: 
        cooldown_epochs: 0

    extra_params:
      loss_type: MSELoss
      enabled_amp: False
      log_step: 20
      predictor_checkpoint_path: None ## for pretrained advective predictor
      autoencoder_checkpoint_path: autoencoder_kl_gan/world_size4-48x48x4_40W/checkpoint_latest.pth ## for pretrained autoencoder
      save_epoch_interval: 20

    wandb:
      project_name: sevir